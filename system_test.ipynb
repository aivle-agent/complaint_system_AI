{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00fada70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install transformers accelerate datasets peft bitsandbytes sentencepiece --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e8fd931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Nov 30 14:59:02 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   38C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b185af5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87f3caad08d64ee8956dbb191294d741",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0eb8ab0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'complaint_system_AI'...\n",
      "remote: Enumerating objects: 15, done.\u001b[K\n",
      "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
      "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
      "remote: Total 15 (delta 2), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (15/15), 12.11 MiB | 40.26 MiB/s, done.\n",
      "Resolving deltas: 100% (2/2), done.\n",
      "/content/complaint_system_AI/complaint_system_AI/complaint_system_AI\n",
      " bandit_prompt_system.py   complain_quality_shap.py   README.md\n",
      " complain_data.zip\t  'Complaint data.zip'\n",
      "Archive:  complain_data.zip\n",
      "  inflating: data/중앙행정기관.csv  \n",
      "  inflating: data/국립아시아문화전당.csv  \n",
      "  inflating: data/지방행정기관.csv  \n",
      "  inflating: data/국민신문고.csv  \n",
      "국립아시아문화전당.csv\t지방행정기관.csv  중앙행정기관.csv  국민신문고.csv\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/aivle-agent/complaint_system_AI.git\n",
    "%cd complaint_system_AI\n",
    "\n",
    "!ls\n",
    "\n",
    "!unzip complain_data.zip -d data\n",
    "\n",
    "!ls data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a19e9b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/중앙행정기관.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117deef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>consulting_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>제목 : 어스앵커 굴착심의 건\\n\\nQ : 아파트 흙막이가시설 공사 중 어스앵커 시...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div><br><label><b>dtype:</b> object</label>"
      ],
      "text/plain": [
       "0    제목 : 어스앵커 굴착심의 건\\n\\nQ : 아파트 흙막이가시설 공사 중 어스앵커 시...\n",
       "Name: consulting_content, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data1.head(1)['consulting_content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7a1bada",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_keys = [\"source\", \"consulting_date\", \"consulting_category\", \"consulting_content\"]\n",
    "\n",
    "# 2) classification_category를 컬럼으로 피벗\n",
    "wide = (\n",
    "    data1.pivot_table(\n",
    "        index=group_keys,\n",
    "        columns=\"classification_category\",\n",
    "        values=\"classification\",\n",
    "        aggfunc=lambda x: \" / \".join(sorted(set(x)))  # 중복 라벨 있으면 합쳐줌\n",
    "    )\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "404cd244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['source', 'consulting_date', 'consulting_category', 'complaint_text',\n",
      "       'outcome', 'summary', 'reason', 'requirement', 'topic'],\n",
      "      dtype='object', name='classification_category')\n",
      "classification_category source  consulting_date consulting_category  \\\n",
      "0                        고용노동부         20140121                   -   \n",
      "1                        고용노동부         20140122                   -   \n",
      "2                        고용노동부         20160218                   -   \n",
      "\n",
      "classification_category                                     complaint_text  \\\n",
      "0                        Q : 3개월 정착 수당 받고 그 후로는 기본급 없이 인센티브로만 급여를 받아왔는데...   \n",
      "1                        Q : 저희 사업장은 2013. 12. 31. 이전까지 퇴직금 누진제가 적용이 되었...   \n",
      "2                        Q : 기타 공공기관 인사 담당자입니다. 저희 기관에서 정규직 직원 채용 시 지원 ...   \n",
      "\n",
      "classification_category   outcome   summary reason requirement topic  \n",
      "0                        추가 상담 필요  일반 민원 상담    민원인    단일 요건 민원    안전  \n",
      "1                           해결 불가  일반 민원 상담    민원인    단일 요건 민원    경제  \n",
      "2                              만족  일반 민원 상담    민원인    단일 요건 민원    경제  \n"
     ]
    }
   ],
   "source": [
    "wide = wide.rename(columns={\n",
    "    \"consulting_content\": \"complaint_text\",\n",
    "    \"상담 주제\": \"topic\",\n",
    "    \"상담 사유\": \"reason\",\n",
    "    \"상담 결과\": \"outcome\",\n",
    "    \"상담 요건\": \"requirement\",\n",
    "    \"상담 내용\": \"summary\",\n",
    "})\n",
    "\n",
    "print(wide.columns)\n",
    "print(wide.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "138299f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1bcbba3b00f440f829645b2e6f80604",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu and disk.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c01af69a36d4d1aa9f948e773ff1f53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Can't find 'adapter_config.json' at './gen_lora'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/config.py\u001b[0m in \u001b[0;36m_get_peft_type\u001b[0;34m(cls, model_id, **hf_hub_download_kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m                 config_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    318\u001b[0m                     \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"repo_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"from_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mvalidate_repo_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mREPO_ID_REGEX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         raise HFValidationError(\n\u001b[0m\u001b[1;32m    161\u001b[0m             \u001b[0;34m\"Repo id must use alphanumeric chars, '-', '_' or '.'.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must use alphanumeric chars, '-', '_' or '.'. The name cannot start or end with '-' or '.' and the maximum length is 96: './gen_lora'.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3053222391.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipython-input-3053222391.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0marms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_default_arms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m     generator = LlamaChatGenerator(\n\u001b[0m\u001b[1;32m    359\u001b[0m         \u001b[0mbase_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"meta-llama/Meta-Llama-3-8B-Instruct\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0mlora_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./gen_lora\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-3053222391.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, base_model, lora_path)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlora_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPeftModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlora_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         self.pipe = transformers.pipeline(\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, model, model_id, adapter_name, is_trainable, config, autocast_adapter_dtype, ephemeral_gpu_offload, low_cpu_mem_usage, key_mapping, **kwargs)\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0muse_auth_token\u001b[0m \u001b[0;34m:=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"use_auth_token\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                 \u001b[0mhf_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"use_auth_token\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_auth_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m             config = PEFT_TYPE_TO_CONFIG_MAPPING[PeftConfig._get_peft_type(model_id, **hf_kwargs)].from_pretrained(\n\u001b[0m\u001b[1;32m    460\u001b[0m                 \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m             )\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/config.py\u001b[0m in \u001b[0;36m_get_peft_type\u001b[0;34m(cls, model_id, **hf_hub_download_kwargs)\u001b[0m\n\u001b[1;32m    321\u001b[0m                 )\n\u001b[1;32m    322\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Can't find '{CONFIG_NAME}' at '{model_id}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0mloaded_attributes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_json_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Can't find 'adapter_config.json' at './gen_lora'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "# 0. LLaMA 파이프라인 초기화\n",
    "\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "llama_pipe = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=MODEL_ID,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# 1. 프롬프트 ARM 정의\n",
    "\n",
    "@dataclass\n",
    "class PromptArmConfig:\n",
    "    \"\"\"\n",
    "    각 프롬프트 전략(arm)에 대한 메타 정보.\n",
    "    feature_vector는 이 전략의 성향을 나타내는 임베딩(스타일/목적 가중치 등).\n",
    "    \"\"\"\n",
    "    name: str\n",
    "    description: str\n",
    "    feature_vector: np.ndarray  # shape = (d,)\n",
    "\n",
    "# 2. Logistic Bandit (arm별 w, H + UCB)\n",
    "\n",
    "class LogisticPromptBandit:\n",
    "    \"\"\"\n",
    "    프롬프트 전략 선택용 logistic bandit.\n",
    "    - 각 arm별로 logistic 모델 w_a, H_a 유지\n",
    "    - UCB 스타일로 exploration (mean + beta * uncertainty)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        arms: List[PromptArmConfig],\n",
    "        lambda_reg: float = 1.0,\n",
    "        eta: float = 0.1,\n",
    "        beta: float = 1.0,\n",
    "    ):\n",
    "        self.arms = arms\n",
    "        self.n_arms = len(arms)\n",
    "        self.d = arms[0].feature_vector.shape[0]\n",
    "\n",
    "        # arm별 parameter, Hessian 근사\n",
    "        self.w = np.zeros((self.n_arms, self.d))  # w_a\n",
    "        self.H = [lambda_reg * np.eye(self.d) for _ in range(self.n_arms)]\n",
    "\n",
    "        self.eta = eta   # step size\n",
    "        self.beta = beta # exploration 강도\n",
    "\n",
    "    def _sigmoid(self, z: float) -> float:\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "    def select_arm(self) -> int:\n",
    "        \"\"\"\n",
    "        현재까지의 w, H를 기반으로 어느 프롬프트 전략 arm을 쓸지 선택.\n",
    "        score_a = (w_a^T x_a) + beta * sqrt( x_a^T H_a^{-1} x_a )\n",
    "        \"\"\"\n",
    "        scores = []\n",
    "        for a_idx, arm in enumerate(self.arms):\n",
    "            x = arm.feature_vector  # (d,)\n",
    "            mean = float(self.w[a_idx].dot(x))\n",
    "\n",
    "            invH = np.linalg.inv(self.H[a_idx])\n",
    "            var = float(x.T @ invH @ x)\n",
    "            ucb = self.beta * np.sqrt(max(var, 1e-12))\n",
    "\n",
    "            scores.append(mean + ucb)\n",
    "\n",
    "        chosen = int(np.argmax(scores))\n",
    "        return chosen\n",
    "\n",
    "    def update(self, arm_idx: int, reward: float):\n",
    "        \"\"\"\n",
    "        bandit reward(0~1 범위)를 받아 online logistic regression update.\n",
    "        reward는 verifier에서 나온 스칼라 점수.\n",
    "        \"\"\"\n",
    "        arm = self.arms[arm_idx]\n",
    "        x = arm.feature_vector\n",
    "        z = float(self.w[arm_idx].dot(x))\n",
    "        p = self._sigmoid(z)  # 현재 model이 보는 \"성공 확률\"\n",
    "\n",
    "        # logistic loss gradient: -(y - p) * x\n",
    "        grad = -(reward - p) * x\n",
    "\n",
    "        # Hessian 근사 업데이트: H_a += x x^T\n",
    "        self.H[arm_idx] += np.outer(x, x)\n",
    "\n",
    "        # Online Newton-style step: w <- w - eta * H^{-1} grad\n",
    "        invH = np.linalg.inv(self.H[arm_idx])\n",
    "        step = self.eta * (invH @ grad)\n",
    "        self.w[arm_idx] -= step\n",
    "\n",
    "    def explain_current_strategy(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        각 arm에 대해 현재 bandit가 보는 \"예상 성공도\"를 텍스트로 정리.\n",
    "        \"\"\"\n",
    "        lines = []\n",
    "        for a_idx, arm in enumerate(self.arms):\n",
    "            x = arm.feature_vector\n",
    "            z = float(self.w[a_idx].dot(x))\n",
    "            p = self._sigmoid(z)\n",
    "            lines.append(\n",
    "                f\"[{arm.name}] \"\n",
    "                f\"예상 성공도 ≈ {p:.3f} | \"\n",
    "                f\"feature={np.round(x, 2)} | \"\n",
    "                f\"설명: {arm.description}\"\n",
    "            )\n",
    "        return lines\n",
    "\n",
    "# 3. LLM Generator (Meta-Llama-3 기반)\n",
    "\n",
    "\n",
    "class LlamaChatGenerator:\n",
    "    def __init__(self, base_model, lora_path=None):\n",
    "        from peft import PeftModel\n",
    "\n",
    "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(base_model)\n",
    "        model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "            base_model, torch_dtype=torch.bfloat16, device_map=\"auto\"\n",
    "        )\n",
    "\n",
    "        if lora_path:\n",
    "            model = PeftModel.from_pretrained(model, lora_path)\n",
    "\n",
    "        self.pipe = transformers.pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "\n",
    "    def generate(self, system_prompt, user_prompt):\n",
    "        msgs = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ]\n",
    "\n",
    "        out = self.pipe(\n",
    "            msgs,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=True,\n",
    "            temperature=0.2,\n",
    "            top_p=0.9,\n",
    "        )[0][\"generated_text\"]\n",
    "\n",
    "        # assistant role만 추출\n",
    "        if isinstance(out, list):\n",
    "            for m in out:\n",
    "                if m.get(\"role\") == \"assistant\":\n",
    "                    return m[\"content\"]\n",
    "        return str(out)\n",
    "\n",
    "\n",
    "\n",
    "def build_prompt_from_arm(\n",
    "    base_instruction: str,\n",
    "    complaint_text: str,\n",
    "    arm: PromptArmConfig,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    arm의 전략 설명을 user 프롬프트에 녹여서 스타일/방향을 제어.\n",
    "    - base_instruction: system 쪽에 들어가는 역할/전략 기본 설명\n",
    "    - 여기서 반환하는 건 user_prompt에 들어갈 텍스트\n",
    "    \"\"\"\n",
    "    return (\n",
    "        f\"아래 민원에 대해 답변을 작성하되, 다음 전략 프로필을 따르세요.\\n\\n\"\n",
    "        f\"[전략 이름]\\n{arm.name}\\n\\n\"\n",
    "        f\"[전략 설명]\\n{arm.description}\\n\\n\"\n",
    "        f\"[민원 내용]\\n{complaint_text}\\n\\n\"\n",
    "        f\"[추가 지침]\\n\"\n",
    "        f\"- 관련 법령/정책과의 일치성을 확인하고, 처리 가능/불가능을 명확히 구분하세요.\\n\"\n",
    "        f\"- 민원인이 이해하기 쉬운 구조로 답변하고, 필요한 경우 대안/절차를 제시하세요.\\n\"\n",
    "        f\"- 과도한 약속이나 확정적인 표현은 피하고, '담당 부서의 최종 판단'이 필요함을 밝혀주세요.\\n\"\n",
    "    )\n",
    "\n",
    "# 4. Verifier: LLM 기반 점수 → reward\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class VerificationScores:\n",
    "    \"\"\"각 품질 지표 스코어 (0~1).\"\"\"\n",
    "    resolution_likelihood: float    # 실제로 해결/처리가 될 가능성\n",
    "    policy_legal_alignment: float   # 정책/법령·과거 답변과의 일치도\n",
    "    explanation_clarity: float      # 구조/논리/명료성\n",
    "    empathy_tone: float             # 민원인 친화적 톤, 갈등 완화\n",
    "    risk_safety: float              # 기관 입장에서의 안전성 (높을수록 안전)\n",
    "\n",
    "\n",
    "class SmallVerifier:\n",
    "    def __init__(self, base_model, lora_path=None):\n",
    "        from peft import PeftModel\n",
    "\n",
    "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(base_model)\n",
    "        model = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "            base_model,\n",
    "            num_labels=1,   # 0~1 점수\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "\n",
    "        if lora_path:\n",
    "            model = PeftModel.from_pretrained(model, lora_path)\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "    def evaluate(self, complaint, answer):\n",
    "        text = f\"[COMPLAINT]\\n{complaint}\\n\\n[ANSWER]\\n{answer}\"\n",
    "        inputs = self.tokenizer(text, truncation=True, max_length=2048, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            score = torch.sigmoid(self.model(**inputs).logits)[0].item()\n",
    "        return score\n",
    "\n",
    "\n",
    "def aggregate_reward(\n",
    "    scores: VerificationScores,\n",
    "    weights: Optional[Dict[str, float]] = None,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    reward 구조:\n",
    "      r = w1 * 해결 가능성\n",
    "        + w2 * 정책/법률 일치\n",
    "        + w3 * 명료성\n",
    "        + w4 * 공감\n",
    "        + w5 * 안전성\n",
    "    \"\"\"\n",
    "    if weights is None:\n",
    "        weights = {\n",
    "            \"resolution_likelihood\": 0.35,\n",
    "            \"policy_legal_alignment\": 0.25,\n",
    "            \"explanation_clarity\": 0.20,\n",
    "            \"empathy_tone\": 0.10,\n",
    "            \"risk_safety\": 0.10,\n",
    "        }\n",
    "\n",
    "    s = (\n",
    "        scores.resolution_likelihood * weights[\"resolution_likelihood\"]\n",
    "        + scores.policy_legal_alignment * weights[\"policy_legal_alignment\"]\n",
    "        + scores.explanation_clarity * weights[\"explanation_clarity\"]\n",
    "        + scores.empathy_tone * weights[\"empathy_tone\"]\n",
    "        + scores.risk_safety * weights[\"risk_safety\"]\n",
    "    )\n",
    "\n",
    "    return float(max(0.0, min(1.0, s)))\n",
    "\n",
    "# 5. 전체 엔진: bandit → generator → verifier → bandit update\n",
    "\n",
    "class PromptBanditEngine:\n",
    "    \"\"\"\n",
    "    - bandit이 프롬프트 전략 선택\n",
    "    - LLM이 답변 생성\n",
    "    - verifier가 품질 측정 → reward 산출\n",
    "    - bandit 업데이트\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        arms: List[PromptArmConfig],\n",
    "        base_instruction: str,\n",
    "        llm: LlamaChatGenerator,\n",
    "        verifier: LlamaVerifier,\n",
    "    ):\n",
    "        self.bandit = LogisticPromptBandit(arms=arms)\n",
    "        self.base_instruction = base_instruction\n",
    "        self.llm = llm\n",
    "        self.verifier = verifier\n",
    "\n",
    "    def step(\n",
    "        self,\n",
    "        complaint_text: str,\n",
    "        reward_weights: Optional[Dict[str, float]] = None,\n",
    "        extra_context: str = \"\",\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        민원 1건에 대해:\n",
    "          1) bandit으로 전략 arm 선택\n",
    "          2) 해당 전략으로 user_prompt 구성 + LLM 답변\n",
    "          3) verifier로 평가 + reward\n",
    "          4) bandit update\n",
    "          5) 결과/로그 반환\n",
    "        \"\"\"\n",
    "        # 1. arm 선택\n",
    "        arm_idx = self.bandit.select_arm()\n",
    "        arm = self.bandit.arms[arm_idx]\n",
    "\n",
    "        # 2. prompt 구성 + LLM 답변\n",
    "        user_prompt = build_prompt_from_arm(\n",
    "            base_instruction=self.base_instruction,\n",
    "            complaint_text=complaint_text,\n",
    "            arm=arm,\n",
    "        )\n",
    "        answer = self.llm.generate(\n",
    "            system_prompt=self.base_instruction,\n",
    "            user_prompt=user_prompt,\n",
    "        )\n",
    "\n",
    "        # 3. verifier 평가\n",
    "        scores = self.verifier.evaluate(\n",
    "            complaint_text=complaint_text,\n",
    "            answer_text=answer,\n",
    "            extra_context=extra_context,\n",
    "        )\n",
    "        reward = aggregate_reward(scores, weights=reward_weights)\n",
    "\n",
    "        # 4. bandit 업데이트\n",
    "        self.bandit.update(arm_idx=arm_idx, reward=reward)\n",
    "\n",
    "        # 5. 현재 전략 방향성 요약\n",
    "        strategy_view = self.bandit.explain_current_strategy()\n",
    "\n",
    "        return {\n",
    "            \"chosen_arm_idx\": arm_idx,\n",
    "            \"chosen_arm_name\": arm.name,\n",
    "            \"chosen_arm_description\": arm.description,\n",
    "            \"user_prompt\": user_prompt,\n",
    "            \"answer\": answer,\n",
    "            \"verification_scores\": scores,\n",
    "            \"reward\": reward,\n",
    "            \"strategy_view\": strategy_view,\n",
    "        }\n",
    "\n",
    "\n",
    "# 6. to be decided\n",
    "\n",
    "def build_default_arms() -> List[PromptArmConfig]:\n",
    "    \"\"\"\n",
    "      [법률/정책 중시 정도, 해결책 제안 정도, 공감/톤, 단호함(제한 강조), 근거 제시 강조]\n",
    "    \"\"\"\n",
    "    return [\n",
    "        PromptArmConfig(\n",
    "            name=\"법률-정책 최우선\",\n",
    "            description=\"법령, 조례, 내부지침과의 일치성을 최우선으로 하고 책임 범위를 분명히 하는 보수적 답변.\",\n",
    "            feature_vector=np.array([0.9, 0.6, 0.4, 0.8, 0.7]),\n",
    "        ),\n",
    "        PromptArmConfig(\n",
    "            name=\"민원인 공감형\",\n",
    "            description=\"민원인의 감정과 상황을 충분히 공감하고, 이해하기 쉬운 언어로 절차와 한계를 설명하는 답변.\",\n",
    "            feature_vector=np.array([0.6, 0.7, 0.9, 0.3, 0.5]),\n",
    "        ),\n",
    "        PromptArmConfig(\n",
    "            name=\"해결책 제안형\",\n",
    "            description=\"현실적으로 가능한 대안과 행동 옵션을 최대한 많이 제시하는 해결 중심 답변.\",\n",
    "            feature_vector=np.array([0.7, 0.9, 0.7, 0.4, 0.8]),\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "\n",
    "def main():\n",
    "    arms = build_default_arms()\n",
    "\n",
    "    generator = LlamaChatGenerator(\n",
    "        base_model=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "        lora_path=\"./gen_lora\"\n",
    "    )\n",
    "\n",
    "    verifier = SmallVerifier(\n",
    "        base_model=\"microsoft/Phi-3-mini-4k-instruct\",\n",
    "        lora_path=\"./verifier_lora\"\n",
    "    )\n",
    "\n",
    "    engine = PromptBanditEngine(\n",
    "        arms=arms,\n",
    "        base_instruction=BASE_SYS_PROMPT,\n",
    "        llm=generator,\n",
    "        verifier=verifier,\n",
    "    )\n",
    "\n",
    "    results = []\n",
    "    for idx, row in tqdm(wide.iterrows(), total=len(wide)):\n",
    "        complaint = row[\"complaint_text\"]\n",
    "        context = row.get(\"topic\", \"\")\n",
    "\n",
    "        out = engine.step(complaint, extra_context=context)\n",
    "\n",
    "        results.append({\n",
    "            \"complaint\": complaint,\n",
    "            \"answer\": out[\"answer\"],\n",
    "            \"reward\": out[\"reward\"],\n",
    "            \"arm\": out[\"chosen_arm_name\"]\n",
    "        })\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results.to_csv(\"full_batch_results.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "     \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1a68d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers peft accelerate bitsandbytes sentencepiece sentence-transformers scikit-learn --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aaf4b8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QA rows: 22410\n",
      "                                            question  \\\n",
      "0  아파트 흙막이가시설 공사 중 어스앵커 시공을 위해 관할구청에 도로점용을 신청하였으나...   \n",
      "1  아파트 흙막이가시설 공사 중 어스앵커 시공을 위해 관할구청에 도로점용을 신청하였으나...   \n",
      "\n",
      "                                              answer source      date  \\\n",
      "0  안녕하십니까? 평소 국토 교통행정에 관심과 애정을 가져 주신 점 깊이 감사드리며, ...  국토교통부  20220922   \n",
      "1  안녕하십니까? 평소 국토 교통행정에 관심과 애정을 가져 주신 점 깊이 감사드리며, ...  국토교통부  20220922   \n",
      "\n",
      "  category classification  \n",
      "0    도로관리과       단일 요건 민원  \n",
      "1    도로관리과  다수기관 복합 민원 상담  \n",
      "Total usable Q/A pairs: 22410\n",
      "Train size: 20169 Test size: 2241\n",
      "Train questions for clustering: 20169\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbcab64e26ad4d6ab51550d2bdb34191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/631 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "클러스터 개수(노이즈 포함): 82\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d51b073ddf4e457ebc3cf7271ebd8240",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "735cee4f57ce473087cbed99c1a8abf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec45b4e540014f5297c5a953b9a69279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a6d47946ee5459abd9e09f4143775a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14a74a431dfe4702b1a86c9532874f6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3588c70037e4bafa2938e6b70ed5717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/599 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6608a30386b94251af26e5a720066ca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c2376572f9641a7a300e7ecc8f4c294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44e2b756c9514ab381b771f9aa1272f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15ce6e2674b242ab8f50f6310cafd27f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b68ba602bb33474a9d357d1678b3ee3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98ab1c5c5e2b4dadb6c40581e7fc8179",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Phi3ForSequenceClassification were not initialized from the model checkpoint at microsoft/Phi-3-mini-4k-instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test set 일부(예: 20개)에 대한 평가 실행 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  0%|          | 0/20 [00:12<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 370.12 MiB is free. Process 74025 has 14.38 GiB memory in use. Of the allocated memory 14.21 GiB is allocated by PyTorch, and 44.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1720239365.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipython-input-1720239365.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[0;31m# 4) Test 일부에 대한 배치 평가\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n Test set 일부(예: 20개)에 대한 평가 실행 중...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m     \u001b[0mdf_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_csv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"batch_results_sample.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"샘플 결과 saved to batch_results_sample.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-1720239365.py\u001b[0m in \u001b[0;36mevaluate_batch\u001b[0;34m(engine, test_data, out_csv)\u001b[0m\n\u001b[1;32m    426\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"question\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m         logs.append({\n\u001b[1;32m    430\u001b[0m             \u001b[0;34m\"question\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-1720239365.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, raw_complaint)\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;31m# 5) Generator 호출 프롬프트 생성\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0muser_prompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_prompt_from_arm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrefined_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBASE_SYS_PROMPT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_prompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0;31m# 6) Verifier 점수 → reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-1720239365.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, system_prompt, user_prompt, max_new_tokens)\u001b[0m\n\u001b[1;32m    293\u001b[0m             \u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0muser_prompt\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         ]\n\u001b[0;32m--> 295\u001b[0;31m         out = self.pipe(\n\u001b[0m\u001b[1;32m    296\u001b[0m             \u001b[0mmsgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0;31m# We have one or more prompts in list-of-dicts format, so this is chat mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_item\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mChat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0mchats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mChat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext_inputs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 🐈 🐈 🐈\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1465\u001b[0m             )\n\u001b[1;32m   1466\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1467\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1469\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mrun_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1472\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1473\u001b[0m         \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpreprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1474\u001b[0;31m         \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1475\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1372\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0minference_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m                     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1375\u001b[0m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"generation_config\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModelOutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2563\u001b[0m         \u001b[0;31m# 9. Call generation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2564\u001b[0;31m         result = decoding_method(\n\u001b[0m\u001b[1;32m   2565\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2566\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2783\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_prefill\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2784\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2785\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2786\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    916\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0;31m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m         \u001b[0mslice_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlogits_to_keep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits_to_keep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlogits_to_keep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnew_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mpre_forward\u001b[0;34m(self, module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    358\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtied_pointers_to_remove\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m                 set_module_tensor_to_device(\n\u001b[0m\u001b[1;32m    361\u001b[0m                     \u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                     \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/utils/modeling.py\u001b[0m in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map, non_blocking, clear_cache)\u001b[0m\n\u001b[1;32m    341\u001b[0m                     \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mold_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m             \u001b[0mnew_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[0mnew_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 370.12 MiB is free. Process 74025 has 14.38 GiB memory in use. Of the allocated memory 14.21 GiB is allocated by PyTorch, and 44.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, re, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Optional\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSequenceClassification\n",
    ")\n",
    "from peft import PeftModel\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "\n",
    "\n",
    "#############################################\n",
    "# 1) 민원 텍스트에서 Q / A 분리\n",
    "#############################################\n",
    "\n",
    "def parse_qa(text: str):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\", \"\"\n",
    "\n",
    "    # Q: ~~~ A: ~~~ 구조에서 Q, A 각각 분리\n",
    "    q_match = re.search(r\"Q\\s*:\\s*(.*?)(?:A\\s*:|$)\", text, flags=re.DOTALL)\n",
    "    a_match = re.search(r\"A\\s*:\\s*(.*)\", text, flags=re.DOTALL)\n",
    "\n",
    "    q = q_match.group(1).strip() if q_match else \"\"\n",
    "    a = a_match.group(1).strip() if a_match else \"\"\n",
    "    return q, a\n",
    "\n",
    "def clean_text(x: Any) -> str:\n",
    "    if not isinstance(x, str):\n",
    "        return \"\"\n",
    "    x = re.sub(r\"\\d{2,3}-\\d{3,4}-\\d{4}\", \"[TEL]\", x)\n",
    "    x = re.sub(r\"\\d{6}-\\d{7}\", \"[RRN]\", x)\n",
    "    x = re.sub(r\"[가-힣]{2,3}씨\", \"[NAME]\", x)\n",
    "    return x.strip()\n",
    "\n",
    "def convert_df_to_qa(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        content = row.get(\"consulting_content\", \"\")\n",
    "        q_raw, a_raw = parse_qa(content)\n",
    "\n",
    "        q = clean_text(q_raw)\n",
    "        a = clean_text(a_raw)\n",
    "\n",
    "        if len(q) < 5 or len(a) < 5:\n",
    "            continue\n",
    "\n",
    "        rows.append({\n",
    "            \"question\": q,\n",
    "            \"answer\": a,\n",
    "            \"source\": row.get(\"source\", \"\"),\n",
    "            \"date\": row.get(\"consulting_date\", \"\"),\n",
    "            \"category\": row.get(\"consulting_category\", \"\"),\n",
    "            \"classification\": row.get(\"classification\", \"\")\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# 4) Dataset Builder (generator/verifier 학습용)\n",
    "\n",
    "\n",
    "def build_dataset(df: pd.DataFrame):\n",
    "    data = []\n",
    "    for _, row in df.iterrows():\n",
    "        q = clean_text(row.get(\"question\", \"\"))\n",
    "        a = clean_text(row.get(\"answer\", \"\"))\n",
    "\n",
    "        if len(q) < 5 or len(a) < 5:\n",
    "            continue\n",
    "\n",
    "        data.append({\"question\": q, \"answer\": a})\n",
    "    return data\n",
    "\n",
    "# 5) Train/Test Split\n",
    "\n",
    "\n",
    "def split_dataset(all_data, test_ratio=0.1):\n",
    "    total = len(all_data)\n",
    "    test_size = max(1, int(total * test_ratio))\n",
    "    return all_data[:-test_size], all_data[-test_size:]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. 프롬프트 전략 ARM 정의\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PromptArmConfig:\n",
    "    name: str\n",
    "    description: str\n",
    "    feature_vector: np.ndarray   \n",
    "\n",
    "\n",
    "def build_prompt_arms() -> List[PromptArmConfig]:\n",
    "    return [\n",
    "        PromptArmConfig(\n",
    "            name=\"법률-정책 최우선\",\n",
    "            description=\"법령·조례·내부지침과의 일치성을 최우선으로 하고, 책임 범위를 명확히 하는 보수적 답변.\",\n",
    "            feature_vector=np.array([0.9, 0.4, 0.5]),\n",
    "        ),\n",
    "        PromptArmConfig(\n",
    "            name=\"민원인 공감형\",\n",
    "            description=\"민원인의 감정과 상황을 충분히 공감하고, 이해하기 쉬운 언어로 절차와 한계를 설명하는 답변.\",\n",
    "            feature_vector=np.array([0.6, 0.9, 0.6]),\n",
    "        ),\n",
    "        PromptArmConfig(\n",
    "            name=\"해결책 제안형\",\n",
    "            description=\"현실적으로 가능한 대안과 행동 옵션을 적극적으로 제시하는 해결 중심 답변.\",\n",
    "            feature_vector=np.array([0.7, 0.6, 0.9]),\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "# 3. Contextual Logistic Bandit\n",
    "#    (arm feature + complaint context feature)\n",
    "# \n",
    "\n",
    "class ContextualLogisticPromptBandit:\n",
    "    \"\"\"\n",
    "    Contextual Bandit:\n",
    "      - 각 arm마다 고정 feature_vector\n",
    "      - 각 민원마다 context_vec (질문 길이, 구체성, 클러스터 등)\n",
    "      - 입력 phi = concat(arm_feature, ctx_vec)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        arms: List[PromptArmConfig],\n",
    "        ctx_dim: int,\n",
    "        lambda_reg: float = 1.0,\n",
    "        eta: float = 0.1,\n",
    "        beta: float = 1.0,\n",
    "    ):\n",
    "        self.arms = arms\n",
    "        self.n_arms = len(arms)\n",
    "        self.d_arm = arms[0].feature_vector.shape[0]\n",
    "        self.d_ctx = ctx_dim\n",
    "        self.d = self.d_arm + self.d_ctx\n",
    "\n",
    "        # arm별 parameter, Hessian 근사\n",
    "        self.w = np.zeros((self.n_arms, self.d))\n",
    "        self.H = [lambda_reg * np.eye(self.d) for _ in range(self.n_arms)]\n",
    "\n",
    "        self.eta = eta\n",
    "        self.beta = beta\n",
    "\n",
    "    def _sigmoid(self, z: float) -> float:\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "    def _phi(self, arm_idx: int, ctx_vec: np.ndarray) -> np.ndarray:\n",
    "        arm_feat = self.arms[arm_idx].feature_vector\n",
    "        return np.concatenate([arm_feat, ctx_vec])  # (d,)\n",
    "\n",
    "    def select_arm(self, ctx_vec: np.ndarray) -> int:\n",
    "        scores = []\n",
    "        for a_idx in range(self.n_arms):\n",
    "            x = self._phi(a_idx, ctx_vec)\n",
    "            mean = float(self.w[a_idx].dot(x))\n",
    "\n",
    "            invH = np.linalg.inv(self.H[a_idx])\n",
    "            var = float(x.T @ invH @ x)\n",
    "            ucb = self.beta * np.sqrt(max(var, 1e-12))\n",
    "\n",
    "            scores.append(mean + ucb)\n",
    "        return int(np.argmax(scores))\n",
    "\n",
    "    def update(self, arm_idx: int, ctx_vec: np.ndarray, reward: float):\n",
    "        x = self._phi(arm_idx, ctx_vec)\n",
    "        z = float(self.w[arm_idx].dot(x))\n",
    "        p = self._sigmoid(z)\n",
    "\n",
    "        grad = -(reward - p) * x\n",
    "        self.H[arm_idx] += np.outer(x, x)\n",
    "\n",
    "        invH = np.linalg.inv(self.H[arm_idx])\n",
    "        step = self.eta * (invH @ grad)\n",
    "        self.w[arm_idx] -= step\n",
    "\n",
    "    def explain_current_strategy(self, ctx_vec: np.ndarray) -> List[str]:\n",
    "        lines = []\n",
    "        for a_idx, arm in enumerate(self.arms):\n",
    "            x = self._phi(a_idx, ctx_vec)\n",
    "            z = float(self.w[a_idx].dot(x))\n",
    "            p = self._sigmoid(z)\n",
    "            lines.append(\n",
    "                f\"[{arm.name}] 예상 성공도 ≈ {p:.3f} | arm_feat={np.round(arm.feature_vector, 2)}\"\n",
    "            )\n",
    "        return lines\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4. Question Refiner (간단 버전; 나중에 LLM으로 교체 가능)\n",
    "# ============================================================\n",
    "\n",
    "class SimpleQuestionRefiner:\n",
    "    \"\"\"\n",
    "    지금은 간단히 공백 정리 + 기본 클리닝만 수행.\n",
    "    나중에 LLM 기반으로 '질문 구조 교정' 로직으로 교체 가능.\n",
    "    \"\"\"\n",
    "    def refine(self, raw_text: str) -> str:\n",
    "        t = clean_text(raw_text)\n",
    "        # TODO: 여기에 LLM 기반 재구성 로직을 넣어도 됨.\n",
    "        return t\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5. DBSCAN 기반 민원 유형화 (임베딩 + 클러스터)\n",
    "# ============================================================\n",
    "\n",
    "class ComplaintClusterer:\n",
    "    \"\"\"\n",
    "    - SentenceTransformer로 임베딩\n",
    "    - DBSCAN으로 군집\n",
    "    - 새로운 민원 → 임베딩 → 가장 유사한 기존 포인트의 클러스터 라벨 사용\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name=\"jhgan/ko-sroberta-multitask\", eps=0.4, min_samples=5):\n",
    "        self.encoder = SentenceTransformer(model_name)\n",
    "        self.dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric=\"cosine\")\n",
    "        self.embeddings = None\n",
    "        self.labels = None\n",
    "\n",
    "    def fit(self, texts: List[str]):\n",
    "        self.embeddings = self.encoder.encode(\n",
    "            texts,\n",
    "            batch_size=32,\n",
    "            show_progress_bar=True,\n",
    "            convert_to_numpy=True,\n",
    "            normalize_embeddings=True,\n",
    "        )\n",
    "        self.dbscan.fit(self.embeddings)\n",
    "        self.labels = self.dbscan.labels_\n",
    "        print(\"클러스터 개수(노이즈 포함):\", len(set(self.labels)))\n",
    "\n",
    "    def predict_cluster(self, text: str) -> int:\n",
    "        if self.embeddings is None or self.labels is None:\n",
    "            return -1\n",
    "        emb = self.encoder.encode([text], convert_to_numpy=True, normalize_embeddings=True)\n",
    "        sims = cosine_similarity(emb, self.embeddings)[0]\n",
    "        idx = int(np.argmax(sims))\n",
    "        return int(self.labels[idx])\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6. Context Vector 생성 (질문 특성 + 클러스터)\n",
    "# ============================================================\n",
    "\n",
    "def build_context_vector(refined_q: str, cluster_id: int) -> np.ndarray:\n",
    "    # 길이 (0~1 스케일)\n",
    "    length = min(len(refined_q) / 500.0, 1.0)\n",
    "    # 문장 수 (0~1 스케일)\n",
    "    num_sent = refined_q.count(\"다.\") + refined_q.count(\".\")\n",
    "    num_sent = min(num_sent / 10.0, 1.0)\n",
    "    # 법률 관련 키워드 여부\n",
    "    has_law = 1.0 if any(k in refined_q for k in [\"법\", \"조항\", \"제\", \"시행령\"]) else 0.0\n",
    "    # 클러스터 id normalize (대충 0~1로)\n",
    "    cluster_norm = 0.0 if cluster_id == -1 else (cluster_id % 20) / 20.0\n",
    "\n",
    "    return np.array([length, num_sent, has_law, cluster_norm], dtype=float)\n",
    "\n",
    "\n",
    "# 7. Generator (LLaMA) + Verifier (Phi-3)\n",
    "\n",
    "\n",
    "class LlamaChatGenerator:\n",
    "    def __init__(self, model_id=\"meta-llama/Meta-Llama-3-8B-Instruct\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "        self.pipe = transformers.pipeline(\n",
    "            \"text-generation\",\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "\n",
    "    def generate(self, system_prompt: str, user_prompt: str, max_new_tokens: int = 256):\n",
    "        msgs = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ]\n",
    "        out = self.pipe(\n",
    "            msgs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.2,\n",
    "            top_p=0.9,\n",
    "        )[0][\"generated_text\"]\n",
    "        return str(out)\n",
    "\n",
    "\n",
    "class PhiMiniVerifier:\n",
    "    \"\"\"\n",
    "    Q/A를 입력으로 받아 0~1 신뢰도 점수 출력.\n",
    "    (지금은 단일 score지만, 나중에 다중 지표 확장 가능)\n",
    "    \"\"\"\n",
    "    def __init__(self, model_id=\"microsoft/Phi-3-mini-4k-instruct\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_id,\n",
    "            num_labels=1,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "\n",
    "    def score(self, question: str, answer: str) -> float:\n",
    "        text = f\"[QUESTION]\\n{question}\\n\\n[ANSWER]\\n{answer}\"\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=2048,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(**inputs).logits\n",
    "            prob = torch.sigmoid(logits)[0].item()\n",
    "        return float(prob)\n",
    "\n",
    "# 8. 프롬프트 조립\n",
    "\n",
    "\n",
    "BASE_SYS_PROMPT = (\n",
    "    \"당신은 한국 공공기관의 민원 보조 AI입니다. \"\n",
    "    \"사실과 법령, 정책과의 일치성, 절차 안내를 중시하며, \"\n",
    "    \"과도한 약속이나 확정적인 표현은 피해야 합니다.\"\n",
    ")\n",
    "\n",
    "def build_prompt_from_arm(\n",
    "    complaint_text: str,\n",
    "    arm: PromptArmConfig,\n",
    ") -> str:\n",
    "    return (\n",
    "        f\"아래 민원에 대해 답변을 작성하되, 제시된 전략 프로필을 따르세요.\\n\\n\"\n",
    "        f\"[전략 이름]\\n{arm.name}\\n\\n\"\n",
    "        f\"[전략 설명]\\n{arm.description}\\n\\n\"\n",
    "        f\"[민원 내용]\\n{complaint_text}\\n\\n\"\n",
    "        f\"[추가 지침]\\n\"\n",
    "        f\"- 관련 법령/정책과의 일치성을 확인하고, 처리 가능/불가능을 명확히 구분하세요.\\n\"\n",
    "        f\"- 민원인이 이해하기 쉬운 구조(요약 → 근거 → 절차 → 주의사항)로 답변하세요.\\n\"\n",
    "        f\"- 과도한 약속이나 확정적인 표현은 피하고, \"\n",
    "        f\"'실제 처리 여부는 담당 부서의 최종 판단에 따릅니다.'라는 문장을 반드시 포함하세요.\\n\"\n",
    "    )\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 9. 전체 엔진: Refiner → Cluster → Bandit → Generator → Verifier\n",
    "# ============================================================\n",
    "\n",
    "class ComplaintEngine:\n",
    "    def __init__(\n",
    "        self,\n",
    "        arms: List[PromptArmConfig],\n",
    "        generator: LlamaChatGenerator,\n",
    "        verifier: PhiMiniVerifier,\n",
    "        clusterer: ComplaintClusterer,\n",
    "        refiner: SimpleQuestionRefiner,\n",
    "        ctx_dim: int,\n",
    "    ):\n",
    "        self.arms = arms\n",
    "        self.generator = generator\n",
    "        self.verifier = verifier\n",
    "        self.clusterer = clusterer\n",
    "        self.refiner = refiner\n",
    "        self.bandit = ContextualLogisticPromptBandit(arms=arms, ctx_dim=ctx_dim)\n",
    "\n",
    "    def step(self, raw_complaint: str) -> Dict[str, Any]:\n",
    "        # 1) 질문 교정\n",
    "        refined_q = self.refiner.refine(raw_complaint)\n",
    "\n",
    "        # 2) DBSCAN 기반 유형 클러스터\n",
    "        cluster_id = self.clusterer.predict_cluster(refined_q)\n",
    "\n",
    "        # 3) Context feature 벡터\n",
    "        ctx_vec = build_context_vector(refined_q, cluster_id)\n",
    "\n",
    "        # 4) Bandit으로 arm 선택\n",
    "        arm_idx = self.bandit.select_arm(ctx_vec)\n",
    "        arm = self.arms[arm_idx]\n",
    "\n",
    "        # 5) Generator 호출 프롬프트 생성\n",
    "        user_prompt = build_prompt_from_arm(refined_q, arm)\n",
    "        answer = self.generator.generate(BASE_SYS_PROMPT, user_prompt)\n",
    "\n",
    "        # 6) Verifier 점수 → reward\n",
    "        reward = self.verifier.score(refined_q, answer)\n",
    "\n",
    "        # 7) Bandit 업데이트\n",
    "        self.bandit.update(arm_idx, ctx_vec, reward)\n",
    "\n",
    "        # 8) 전략 요약 (현재 context 기준 추정)\n",
    "        strat_view = self.bandit.explain_current_strategy(ctx_vec)\n",
    "\n",
    "        return {\n",
    "            \"raw_complaint\": raw_complaint,\n",
    "            \"refined_complaint\": refined_q,\n",
    "            \"cluster_id\": cluster_id,\n",
    "            \"context_vec\": ctx_vec,\n",
    "            \"chosen_arm_idx\": arm_idx,\n",
    "            \"chosen_arm_name\": arm.name,\n",
    "            \"answer\": answer,\n",
    "            \"reward\": reward,\n",
    "            \"strategy_view\": strat_view,\n",
    "        }\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 10. 배치 평가 (Train/Test 중 Test 파트에 대해)\n",
    "# ============================================================\n",
    "\n",
    "def evaluate_batch(engine: ComplaintEngine, test_data, out_csv=\"batch_results.csv\"):\n",
    "    logs = []\n",
    "    for item in tqdm(test_data):\n",
    "        q = item[\"question\"]\n",
    "        out = engine.step(q)\n",
    "        logs.append({\n",
    "            \"question\": q,\n",
    "            \"refined_question\": out[\"refined_complaint\"],\n",
    "            \"cluster_id\": out[\"cluster_id\"],\n",
    "            \"arm\": out[\"chosen_arm_name\"],\n",
    "            \"score\": out[\"reward\"],\n",
    "            \"answer\": out[\"answer\"],\n",
    "        })\n",
    "    df = pd.DataFrame(logs)\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 11. 실시간 서비스용 래퍼\n",
    "# ============================================================\n",
    "\n",
    "def ask(engine: ComplaintEngine, complaint: str):\n",
    "    out = engine.step(complaint)\n",
    "    print(\"========================================\")\n",
    "    print(\"[선택된 전략] \", out[\"chosen_arm_name\"])\n",
    "    print(\"[클러스터 ID]\", out[\"cluster_id\"])\n",
    "    print(\"[컨텍스트 벡터]\", np.round(out[\"context_vec\"], 3))\n",
    "    print(\"\\n[교정된 민원]\\n\", out[\"refined_complaint\"])\n",
    "    print(\"\\n[예상 답변 앞부분]\\n\", out[\"answer\"][:800])\n",
    "    print(\"\\n[Verifier 기반 신뢰도 score] \", round(out[\"reward\"], 3))\n",
    "    print(\"\\n[현재 Bandit 전략 뷰]\")\n",
    "    for line in out[\"strategy_view\"]:\n",
    "        print(\"  \", line)\n",
    "    print(\"========================================\")\n",
    "    return out\n",
    "\n",
    "\n",
    "# 12. MAIN: 전체 흐름 연결\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 0) 원본 CSV 로드\n",
    "    raw = pd.read_csv('data/중앙행정기관.csv')\n",
    "\n",
    "    # 1) Q/A 분리된 DF 생성\n",
    "    qa_df = convert_df_to_qa(raw)\n",
    "    print(\"QA rows:\", len(qa_df))\n",
    "    print(qa_df.head(2))\n",
    "\n",
    "    # 2) build_dataset + train/test split\n",
    "    all_data = build_dataset(qa_df)\n",
    "    print(\"Total usable Q/A pairs:\", len(all_data))\n",
    "\n",
    "    train_data, test_data = split_dataset(all_data, test_ratio=0.1)\n",
    "    print(\"Train size:\", len(train_data), \"Test size:\", len(test_data))\n",
    "\n",
    "    # 3) 클러스터링에 쓸 질문 텍스트\n",
    "    train_questions = [d[\"question\"] for d in train_data]\n",
    "\n",
    "    print(\"Train questions for clustering:\", len(train_questions))\n",
    "    if len(train_questions) == 0:\n",
    "        raise RuntimeError(\"No questions available for clustering. Check parsing/filters.\")\n",
    "\n",
    "    # 4) 클러스터러 학습\n",
    "    clusterer = ComplaintClusterer()\n",
    "    clusterer.fit(train_questions)\n",
    "\n",
    "    # 3) 컴포넌트 초기화\n",
    "    arms = build_prompt_arms()\n",
    "    generator = LlamaChatGenerator(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "    verifier = PhiMiniVerifier(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "    refiner = SimpleQuestionRefiner()\n",
    "\n",
    "    engine = ComplaintEngine(\n",
    "        arms=arms,\n",
    "        generator=generator,\n",
    "        verifier=verifier,\n",
    "        clusterer=clusterer,\n",
    "        refiner=refiner,\n",
    "        ctx_dim=4,   # build_context_vector에서 4차원 벡터 생성\n",
    "    )\n",
    "\n",
    "    # 4) Test 일부에 대한 배치 평가\n",
    "    print(\"\\n Test set 일부(예: 20개)에 대한 평가 실행 중...\")\n",
    "    df_res = evaluate_batch(engine, test_data[:20], out_csv=\"batch_results_sample.csv\")\n",
    "    print(\"샘플 결과 saved to batch_results_sample.csv\")\n",
    "    display(df_res.head())\n",
    "\n",
    "    # 5) 실시간 예시\n",
    "    print(\"\\n 실시간 예시 민원 입력 테스트\")\n",
    "    _ = ask(engine, \"우리 아파트 단지 앞 도로에 불법주차 차량이 많아서 사고 위험이 큽니다. 어떤 조치를 받을 수 있나요?\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
